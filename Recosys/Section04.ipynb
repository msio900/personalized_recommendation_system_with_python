{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 SGD를 사용한 MF 기본 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "base_src =  './Data'\n",
    "u_data_src = os.path.join(base_src, 'u.data')\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv(u_data_src,\n",
    "                        sep='\\t',\n",
    "                        names=r_cols,\n",
    "                        encoding='latin-1')\n",
    "\n",
    "# timestamp 제거\n",
    "ratings = ratings[['user_id', 'movie_id', 'rating']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 10 ; train RMSE = 0.9588\n",
      "Iteration : 20 ; train RMSE = 0.9380\n",
      "Iteration : 30 ; train RMSE = 0.9291\n",
      "Iteration : 40 ; train RMSE = 0.9241\n",
      "Iteration : 50 ; train RMSE = 0.9208\n",
      "Iteration : 60 ; train RMSE = 0.9185\n",
      "Iteration : 70 ; train RMSE = 0.9166\n",
      "Iteration : 80 ; train RMSE = 0.9150\n",
      "Iteration : 90 ; train RMSE = 0.9135\n",
      "Iteration : 100 ; train RMSE = 0.9120\n"
     ]
    }
   ],
   "source": [
    "class MF():\n",
    "    def __init__(self, ratings, hyper_params):\n",
    "        self.R = np.array(ratings)\n",
    "        self.num_users, self.num_items = np.shape(self.R)\n",
    "        self.K = hyper_params['K']\n",
    "        self.alpha = hyper_params['alpha']\n",
    "        self.beta = hyper_params['beta']\n",
    "        self.iterations = hyper_params['iterations']\n",
    "        self.verbose = hyper_params['verbose']\n",
    "\n",
    "    def rmse(self):\n",
    "        xs, ys = self.R.nonzero()\n",
    "        self.predictions = []\n",
    "        self.errors = []\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            prediction = self.get_prediction(x, y)\n",
    "            self.predictions.append(prediction)\n",
    "            self.errors.append(self.R[x, y] - prediction)\n",
    "        self.predictions = np.array(self.predictions)\n",
    "        self.errors = np.array(self.errors)\n",
    "\n",
    "        return np.sqrt(np.mean(self.errors**2))\n",
    "\n",
    "    def train(self):\n",
    "        self.P = np.random.normal(scale=1./self.K,                  # scale = 표준편차를 이야기함\n",
    "                                    size=(self.num_users, self.K))  # size 실제 유저수와 잠재요인의 갯수 = 크기 값\n",
    "        self.Q = np.random.normal(scale=1./self.K,\n",
    "                                    size = (self.num_items, self.K))\n",
    "\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_d = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[self.R.nonzero()])\n",
    "\n",
    "        rows, columns = self.R.nonzero()\n",
    "        self.samples = [(i, j, self.R[i, j]) for i, j in zip(rows, columns)]\n",
    "\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            rmse = self.rmse()\n",
    "            training_process.append((i+1, rmse)) # 몇번 째의 RMSE인지?\n",
    "            if self.verbose:\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print('Iteration : %d ; train RMSE = %.4f'%(i + 1, rmse))\n",
    "        return training_process\n",
    "\n",
    "    def get_prediction(self, i, j):\n",
    "        prediction = self.b + self.b_u[i] +self.b_d[j] + self.P[i, :].dot(self.Q[j,].T)\n",
    "        return prediction\n",
    "    def sgd(self):\n",
    "        for i, j, r in self.samples:\n",
    "            prediction = self.get_prediction(i, j)\n",
    "            e = (r-prediction)\n",
    "\n",
    "            self.b_u[i] += self.alpha * (e - (self.beta * self.b_u[i]))\n",
    "            self.b_d[j] += self.alpha * (e - (self.beta * self.b_d[j]))\n",
    "\n",
    "            self.P[i,:] += self.alpha * ((e * self.Q[j,:] - (self.beta * self.P[i, :])))\n",
    "            self.Q[j,:] += self.alpha * ((e * self.Q[i,:] - (self.beta * self.Q[j, :])))\n",
    "\n",
    "R_temp = ratings.pivot(index='user_id',\\\n",
    "                        columns='movie_id',\n",
    "                        values='rating').fillna(0)\n",
    "\n",
    "hyper_params = {\n",
    "    'K' : 30,\n",
    "    'alpha' : 0.001,\n",
    "    'beta' : 0.02,\n",
    "    'iterations' :100,\n",
    "    'verbose' : True\n",
    "}\n",
    "\n",
    "mf = MF(R_temp, hyper_params)\n",
    "\n",
    "train_process = mf.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 train/test 분리 MF 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; Train RMSE = 0.9666 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9412 ; Test RMSE = 0.9623\n",
      "Iteration: 30 ; Train RMSE = 0.9297 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9228 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9179 ; Test RMSE = 0.9493\n",
      "Iteration: 60 ; Train RMSE = 0.9139 ; Test RMSE = 0.9478\n",
      "Iteration: 70 ; Train RMSE = 0.9101 ; Test RMSE = 0.9466\n",
      "Iteration: 80 ; Train RMSE = 0.9059 ; Test RMSE = 0.9455\n",
      "Iteration: 90 ; Train RMSE = 0.9008 ; Test RMSE = 0.9442\n",
      "Iteration: 100 ; Train RMSE = 0.8941 ; Test RMSE = 0.9425\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "base_src =  './Data'\n",
    "u_data_src = os.path.join(base_src, 'u.data')\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv(u_data_src,\n",
    "                        sep='\\t',\n",
    "                        names=r_cols,\n",
    "                        encoding='latin-1')\n",
    "\n",
    "# timestamp 제거\n",
    "ratings = ratings[['user_id', 'movie_id', 'rating']].astype(int)\n",
    "\n",
    "# train / test set 분리\n",
    "from sklearn.utils import shuffle\n",
    "TRAIN_SIZE = 0.75\n",
    "\n",
    "# (사용자 - 영화 - 평점)\n",
    "ratings = shuffle(ratings, random_state=2021)   # 모든 사람이 똑같은 raitings가 나오게 됨. \n",
    "cutoff = int(TRAIN_SIZE * len(ratings))\n",
    "ratings_train = ratings.iloc[:cutoff]           # 어디까지 \n",
    "ratings_test = ratings.iloc[cutoff:]            # 어디 이후로 데이터를 뽑으면 될것 인지?\n",
    "\n",
    "class NEW_MF():\n",
    "    def __init__(self, ratings, hyper_params):\n",
    "        self.R = np.array(ratings)\n",
    "        # 사용자 수 (num_users)와 아이템 수 (num_items)를 받아온다.\n",
    "        self.num_users, self.num_items = np.shape(self.R)\n",
    "        # 아래는 MF weight 조절을 위한 하이퍼 파라미더다.\n",
    "        # K : 잠재요인(latent factor)의 수\n",
    "        self.K = hyper_params['K']\n",
    "        # alpha : 학습률\n",
    "        self.alpha = hyper_params['alpha']\n",
    "        # beta : 정규화 개수\n",
    "        self.beta = hyper_params['beta']\n",
    "        # iterations : SGD의 계산을 할 때 반복 횟수\n",
    "        self.iterations = hyper_params['iterations']\n",
    "        # verbose : SGD의 학습 과정을 중간중간에 출력할 것인지에 대한 여부\n",
    "        self.verbose = hyper_params['verbose']\n",
    "\n",
    "        # 지난 시간과 조금 다른 부분\n",
    "        # movie_lens 데이터는 굉장히 잘 정리된 데이터 이지만, 현업의 데이터는 연속값이 아닐 수 있음. \n",
    "        # self.R을 numpy로 변환 시킬 경우 중간에 비어있는 실제 id 랑 self.R의 값과 매칭이 안됨.\n",
    "        ### Item id에 관한 ###\n",
    "        item_id_index = []\n",
    "        index_item_id = []\n",
    "        for i, one_id in enumerate(ratings):\n",
    "            item_id_index.append([one_id, i])\n",
    "            index_item_id.append([i, one_id])\n",
    "        self.item_id_index = dict(item_id_index)    # 어떤 id 값이 들어오더라도 id 값과 numpy array와 매핑 시켜줌.\n",
    "        self.index_item_id = dict(index_item_id)\n",
    "\n",
    "        user_id_index = []\n",
    "        index_user_id = []\n",
    "        for i, one_id in enumerate(ratings.T):\n",
    "            user_id_index.append([one_id, i])\n",
    "            index_user_id.append([i, one_id])\n",
    "        self.user_id_index = dict(user_id_index)\n",
    "        self.index_user_id = dict(index_user_id)\n",
    "\n",
    "\n",
    "    def rmse(self):\n",
    "        # self.R에서 평점이 있는 (0이 아닌) 요소의 인덱스를 가져온다.\n",
    "        xs, ys = self.R.nonzero()\n",
    "        # prediction과 error를 담을 리스트 변수 초기화\n",
    "        self.predictions = []\n",
    "        self.errors = []\n",
    "        # 평점이 있는 요소(사용자 x, 아이템 y) 각각에 대해서 아래의 코드를 실행한다.\n",
    "        for x, y in zip(xs, ys):\n",
    "            # 사용자 x, 아이템 y에 대해서 평점 예측치를 get_prediction()함수를 사용해서 계산한다.\n",
    "            prediction = self.get_prediction(x, y)\n",
    "            # 예측값을 예측값 리스트에 추가한다.\n",
    "            self.predictions.append(prediction)\n",
    "            # 실제값(R)과 예측값의 차이(errors) 계산해서 오차값 리스트에 추가한다.\n",
    "            self.errors.append(self.R[x, y] - prediction)\n",
    "        # 예측값 리스트와 오차값 리스트를 numpy array형태로 변환한다.\n",
    "        self.predictions = np.array(self.predictions)\n",
    "        self.errors = np.array(self.errors)\n",
    "        # error를 활용해서 RMSE를 도출\n",
    "        return np.sqrt(np.mean(self.errors**2))\n",
    "\n",
    "    def sgd(self):\n",
    "        for i, j, r in self.samples:\n",
    "            # 사용자 i : 아이템 j에 대한 평점 예측치 계산\n",
    "            prediction = self.get_prediction(i, j)\n",
    "            # 실제 평점과 비교한 오차 계산\n",
    "            e = (r - prediction)\n",
    "            # 사용자 평가 경향 계산 및 업데이트\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            # 아이템 평가 경향 계산 및 업데이트\n",
    "            self.b_d[j] += self.alpha * (e - self.beta * self.b_d[j])\n",
    "            # P 행렬 계산 및 업데이트\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            # Q 행렬 계산 및 업데이트\n",
    "            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:])\n",
    "    def get_prediction(self, i, j):\n",
    "        # 사용자 i, 아이템 j에 대한 평점 예측치를 앞에서 배웠던 식을 이용해서 구한다.\n",
    "        prediction = self.b + self.b_u[i] + self.b_d[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "\n",
    "    # Test set 선정\n",
    "    def set_test(self, ratings_test):\n",
    "        test_set = []\n",
    "        for i in range(len(ratings_test)):      # test 데이터에 있는 각 데이터에 대해서\n",
    "            x = self.user_id_index[ratings_test.iloc[i, 0]]\n",
    "            y = self.item_id_index[ratings_test.iloc[i, 1]]\n",
    "            z = ratings_test.iloc[i, 2]\n",
    "            test_set.append([x, y, z])\n",
    "            self.R[x, y] = 0                    # test set으로 지정한 것들은 모두 0으로\n",
    "        self.test_set = test_set\n",
    "        return test_set                   \n",
    "\n",
    "\n",
    "    # Test set RMSE 계산\n",
    "    def test_rmse(self):\n",
    "        error = 0   # 0으로 초기화\n",
    "        for one_set in self.test_set:\n",
    "            predicted = self.get_prediction(one_set[0], one_set[1])\n",
    "            error += pow(one_set[2] - predicted, 2)         # pow : e => e^2 차승\n",
    "        return np.sqrt(error/len(self.test_set))\n",
    "\n",
    "    def test(self):\n",
    "        # Initializing user-feature and item-feature matrix\n",
    "        self.P = np.random.normal(scale=1./self.K,\n",
    "                                    size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, \n",
    "                                    size=(self.num_items, self.K))\n",
    "\n",
    "        # 유저 경향\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_d = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[self.R.nonzero()]) # 온전히 걸린것들만 계산하게 함. \n",
    "\n",
    "        # List of training samples\n",
    "        rows, columns = self.R.nonzero()            # non zero인 것만 인덱스를 가져옴.\n",
    "        self.samples = [(i, j, self.R[i,j]) for i, j in zip(rows, columns)]\n",
    "\n",
    "        # Stochastic gradient descent for given number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            rmse1 = self.rmse()\n",
    "            rmse2 = self.test_rmse()\n",
    "            training_process.append((i+1, rmse1, rmse2))\n",
    "            if self.verbose:\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(\"Iteration: %d ; Train RMSE = %.4f ; Test RMSE = %.4f\" % (i+1, rmse1, rmse2))\n",
    "        return training_process\n",
    "\n",
    "    def get_one_prediction(self, user_id, item_id):\n",
    "        return self.get_prediction(self.user_id_index[user_id],\n",
    "                                    self.item_id_index[item_id])    # 예측치를 계산해줌.\n",
    "    \n",
    "    # Full user-movie rating matrix\n",
    "    def full_prediction(self):\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_d[np.newaxis,:] + self.P.dot(self.Q.T) # 전체를 계산해줌.\n",
    "\n",
    "R_temp = ratings.pivot(index='user_id',\n",
    "                    columns = 'movie_id',\n",
    "                    values = 'rating').fillna(0)\n",
    "\n",
    "hyper_params = {\n",
    "    'K' : 30,\n",
    "    'alpha' : 0.001,\n",
    "    'beta' : 0.02,\n",
    "    'iterations' : 100,\n",
    "    'verbose' : True\n",
    "}\n",
    "\n",
    "mf = NEW_MF(R_temp, hyper_params)\n",
    "\n",
    "test_set = mf.set_test(ratings_test)\n",
    "results = mf.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.91454543 3.39333233 2.98759373 ... 3.38110623 3.46510384 3.41074121]\n",
      " [3.80528174 3.24899288 2.90001427 ... 3.26739582 3.36065561 3.33977482]\n",
      " [3.40858422 2.91780662 2.49550962 ... 2.88229575 2.9774935  2.94997816]\n",
      " ...\n",
      " [4.14704511 3.60480179 3.24708529 ... 3.58197039 3.70818832 3.69413186]\n",
      " [4.32293351 3.78026638 3.40714565 ... 3.75817252 3.89115358 3.86689988]\n",
      " [3.83928592 3.36202847 2.94700558 ... 3.29387556 3.42047262 3.39708125]]\n"
     ]
    }
   ],
   "source": [
    "print(mf.full_prediction())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3933323287310477\n"
     ]
    }
   ],
   "source": [
    "print(mf.get_one_prediction(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 MF의 최적 파라미터 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K : 50\n",
      "Iteration: 10 ; Train RMSE = 0.9669 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9417 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9305 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9239 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9195 ; Test RMSE = 0.9493\n",
      "Iteration: 60 ; Train RMSE = 0.9160 ; Test RMSE = 0.9479\n",
      "Iteration: 70 ; Train RMSE = 0.9129 ; Test RMSE = 0.9467\n",
      "Iteration: 80 ; Train RMSE = 0.9097 ; Test RMSE = 0.9458\n",
      "Iteration: 90 ; Train RMSE = 0.9060 ; Test RMSE = 0.9447\n",
      "Iteration: 100 ; Train RMSE = 0.9012 ; Test RMSE = 0.9432\n",
      "K : 60\n",
      "Iteration: 10 ; Train RMSE = 0.9669 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9418 ; Test RMSE = 0.9623\n",
      "Iteration: 30 ; Train RMSE = 0.9307 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9242 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9198 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9165 ; Test RMSE = 0.9479\n",
      "Iteration: 70 ; Train RMSE = 0.9137 ; Test RMSE = 0.9469\n",
      "Iteration: 80 ; Train RMSE = 0.9108 ; Test RMSE = 0.9461\n",
      "Iteration: 90 ; Train RMSE = 0.9076 ; Test RMSE = 0.9451\n",
      "Iteration: 100 ; Train RMSE = 0.9035 ; Test RMSE = 0.9440\n",
      "K : 70\n",
      "Iteration: 10 ; Train RMSE = 0.9670 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9419 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9308 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9244 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9201 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9169 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9143 ; Test RMSE = 0.9470\n",
      "Iteration: 80 ; Train RMSE = 0.9117 ; Test RMSE = 0.9463\n",
      "Iteration: 90 ; Train RMSE = 0.9089 ; Test RMSE = 0.9455\n",
      "Iteration: 100 ; Train RMSE = 0.9055 ; Test RMSE = 0.9446\n",
      "K : 80\n",
      "Iteration: 10 ; Train RMSE = 0.9670 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9420 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9309 ; Test RMSE = 0.9551\n",
      "Iteration: 40 ; Train RMSE = 0.9245 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9203 ; Test RMSE = 0.9493\n",
      "Iteration: 60 ; Train RMSE = 0.9171 ; Test RMSE = 0.9479\n",
      "Iteration: 70 ; Train RMSE = 0.9145 ; Test RMSE = 0.9470\n",
      "Iteration: 80 ; Train RMSE = 0.9120 ; Test RMSE = 0.9461\n",
      "Iteration: 90 ; Train RMSE = 0.9093 ; Test RMSE = 0.9453\n",
      "Iteration: 100 ; Train RMSE = 0.9059 ; Test RMSE = 0.9443\n",
      "K : 90\n",
      "Iteration: 10 ; Train RMSE = 0.9670 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9420 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9310 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9247 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9205 ; Test RMSE = 0.9493\n",
      "Iteration: 60 ; Train RMSE = 0.9174 ; Test RMSE = 0.9479\n",
      "Iteration: 70 ; Train RMSE = 0.9149 ; Test RMSE = 0.9470\n",
      "Iteration: 80 ; Train RMSE = 0.9126 ; Test RMSE = 0.9462\n",
      "Iteration: 90 ; Train RMSE = 0.9101 ; Test RMSE = 0.9454\n",
      "Iteration: 100 ; Train RMSE = 0.9071 ; Test RMSE = 0.9445\n",
      "K : 100\n",
      "Iteration: 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9421 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9311 ; Test RMSE = 0.9551\n",
      "Iteration: 40 ; Train RMSE = 0.9247 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9206 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9175 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9151 ; Test RMSE = 0.9470\n",
      "Iteration: 80 ; Train RMSE = 0.9129 ; Test RMSE = 0.9462\n",
      "Iteration: 90 ; Train RMSE = 0.9105 ; Test RMSE = 0.9455\n",
      "Iteration: 100 ; Train RMSE = 0.9077 ; Test RMSE = 0.9447\n",
      "K : 110\n",
      "Iteration: 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9421 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9311 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9248 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9207 ; Test RMSE = 0.9493\n",
      "Iteration: 60 ; Train RMSE = 0.9177 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9153 ; Test RMSE = 0.9470\n",
      "Iteration: 80 ; Train RMSE = 0.9131 ; Test RMSE = 0.9463\n",
      "Iteration: 90 ; Train RMSE = 0.9108 ; Test RMSE = 0.9455\n",
      "Iteration: 100 ; Train RMSE = 0.9081 ; Test RMSE = 0.9447\n",
      "K : 120\n",
      "Iteration: 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9421 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9312 ; Test RMSE = 0.9551\n",
      "Iteration: 40 ; Train RMSE = 0.9249 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9207 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9178 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9154 ; Test RMSE = 0.9470\n",
      "Iteration: 80 ; Train RMSE = 0.9132 ; Test RMSE = 0.9463\n",
      "Iteration: 90 ; Train RMSE = 0.9110 ; Test RMSE = 0.9456\n",
      "Iteration: 100 ; Train RMSE = 0.9083 ; Test RMSE = 0.9447\n",
      "K : 130\n",
      "Iteration: 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9422 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9312 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9249 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9208 ; Test RMSE = 0.9493\n",
      "Iteration: 60 ; Train RMSE = 0.9179 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9155 ; Test RMSE = 0.9470\n",
      "Iteration: 80 ; Train RMSE = 0.9134 ; Test RMSE = 0.9463\n",
      "Iteration: 90 ; Train RMSE = 0.9113 ; Test RMSE = 0.9456\n",
      "Iteration: 100 ; Train RMSE = 0.9088 ; Test RMSE = 0.9448\n",
      "K : 140\n",
      "Iteration: 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9422 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9312 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9250 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9209 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9180 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9157 ; Test RMSE = 0.9471\n",
      "Iteration: 80 ; Train RMSE = 0.9136 ; Test RMSE = 0.9463\n",
      "Iteration: 90 ; Train RMSE = 0.9116 ; Test RMSE = 0.9457\n",
      "Iteration: 100 ; Train RMSE = 0.9092 ; Test RMSE = 0.9449\n",
      "K : 150\n",
      "Iteration: 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9422 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9312 ; Test RMSE = 0.9551\n",
      "Iteration: 40 ; Train RMSE = 0.9250 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9209 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9180 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9158 ; Test RMSE = 0.9471\n",
      "Iteration: 80 ; Train RMSE = 0.9138 ; Test RMSE = 0.9464\n",
      "Iteration: 90 ; Train RMSE = 0.9118 ; Test RMSE = 0.9458\n",
      "Iteration: 100 ; Train RMSE = 0.9096 ; Test RMSE = 0.9451\n",
      "K : 160\n",
      "Iteration: 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9422 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9551\n",
      "Iteration: 40 ; Train RMSE = 0.9250 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9210 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9181 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9159 ; Test RMSE = 0.9471\n",
      "Iteration: 80 ; Train RMSE = 0.9139 ; Test RMSE = 0.9464\n",
      "Iteration: 90 ; Train RMSE = 0.9120 ; Test RMSE = 0.9458\n",
      "Iteration: 100 ; Train RMSE = 0.9099 ; Test RMSE = 0.9451\n",
      "K : 170\n",
      "Iteration: 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9422 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9251 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9210 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9182 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9160 ; Test RMSE = 0.9471\n",
      "Iteration: 80 ; Train RMSE = 0.9141 ; Test RMSE = 0.9464\n",
      "Iteration: 90 ; Train RMSE = 0.9122 ; Test RMSE = 0.9458\n",
      "Iteration: 100 ; Train RMSE = 0.9101 ; Test RMSE = 0.9451\n",
      "K : 180\n",
      "Iteration: 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9422 ; Test RMSE = 0.9623\n",
      "Iteration: 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9551\n",
      "Iteration: 40 ; Train RMSE = 0.9251 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9211 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9182 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9160 ; Test RMSE = 0.9471\n",
      "Iteration: 80 ; Train RMSE = 0.9141 ; Test RMSE = 0.9464\n",
      "Iteration: 90 ; Train RMSE = 0.9123 ; Test RMSE = 0.9458\n",
      "Iteration: 100 ; Train RMSE = 0.9103 ; Test RMSE = 0.9452\n",
      "K : 190\n",
      "Iteration: 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9423 ; Test RMSE = 0.9623\n",
      "Iteration: 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9251 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9211 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9183 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9161 ; Test RMSE = 0.9471\n",
      "Iteration: 80 ; Train RMSE = 0.9143 ; Test RMSE = 0.9465\n",
      "Iteration: 90 ; Train RMSE = 0.9125 ; Test RMSE = 0.9459\n",
      "Iteration: 100 ; Train RMSE = 0.9107 ; Test RMSE = 0.9453\n",
      "K : 200\n",
      "Iteration: 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9423 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9251 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9211 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9183 ; Test RMSE = 0.9481\n",
      "Iteration: 70 ; Train RMSE = 0.9162 ; Test RMSE = 0.9471\n",
      "Iteration: 80 ; Train RMSE = 0.9144 ; Test RMSE = 0.9465\n",
      "Iteration: 90 ; Train RMSE = 0.9127 ; Test RMSE = 0.9459\n",
      "Iteration: 100 ; Train RMSE = 0.9109 ; Test RMSE = 0.9454\n",
      "K : 210\n",
      "Iteration: 10 ; Train RMSE = 0.9671 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9423 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9314 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9252 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9212 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9184 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9162 ; Test RMSE = 0.9471\n",
      "Iteration: 80 ; Train RMSE = 0.9144 ; Test RMSE = 0.9464\n",
      "Iteration: 90 ; Train RMSE = 0.9127 ; Test RMSE = 0.9459\n",
      "Iteration: 100 ; Train RMSE = 0.9109 ; Test RMSE = 0.9454\n",
      "K : 220\n",
      "Iteration: 10 ; Train RMSE = 0.9672 ; Test RMSE = 0.9806\n",
      "Iteration: 20 ; Train RMSE = 0.9423 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9314 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9252 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9212 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9184 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9163 ; Test RMSE = 0.9471\n",
      "Iteration: 80 ; Train RMSE = 0.9145 ; Test RMSE = 0.9465\n",
      "Iteration: 90 ; Train RMSE = 0.9128 ; Test RMSE = 0.9459\n",
      "Iteration: 100 ; Train RMSE = 0.9111 ; Test RMSE = 0.9454\n",
      "K : 230\n",
      "Iteration: 10 ; Train RMSE = 0.9672 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9423 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9314 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9252 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9212 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9184 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9163 ; Test RMSE = 0.9471\n",
      "Iteration: 80 ; Train RMSE = 0.9145 ; Test RMSE = 0.9465\n",
      "Iteration: 90 ; Train RMSE = 0.9129 ; Test RMSE = 0.9459\n",
      "Iteration: 100 ; Train RMSE = 0.9111 ; Test RMSE = 0.9454\n",
      "K : 240\n",
      "Iteration: 10 ; Train RMSE = 0.9672 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9423 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9314 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9252 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9212 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9184 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9163 ; Test RMSE = 0.9471\n",
      "Iteration: 80 ; Train RMSE = 0.9146 ; Test RMSE = 0.9465\n",
      "Iteration: 90 ; Train RMSE = 0.9130 ; Test RMSE = 0.9460\n",
      "Iteration: 100 ; Train RMSE = 0.9113 ; Test RMSE = 0.9455\n",
      "K : 250\n",
      "Iteration: 10 ; Train RMSE = 0.9672 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9423 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9314 ; Test RMSE = 0.9551\n",
      "Iteration: 40 ; Train RMSE = 0.9252 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9213 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9185 ; Test RMSE = 0.9481\n",
      "Iteration: 70 ; Train RMSE = 0.9164 ; Test RMSE = 0.9471\n",
      "Iteration: 80 ; Train RMSE = 0.9147 ; Test RMSE = 0.9465\n",
      "Iteration: 90 ; Train RMSE = 0.9131 ; Test RMSE = 0.9460\n",
      "Iteration: 100 ; Train RMSE = 0.9114 ; Test RMSE = 0.9454\n",
      "K : 260\n",
      "Iteration: 10 ; Train RMSE = 0.9672 ; Test RMSE = 0.9807\n",
      "Iteration: 20 ; Train RMSE = 0.9423 ; Test RMSE = 0.9622\n",
      "Iteration: 30 ; Train RMSE = 0.9314 ; Test RMSE = 0.9552\n",
      "Iteration: 40 ; Train RMSE = 0.9252 ; Test RMSE = 0.9515\n",
      "Iteration: 50 ; Train RMSE = 0.9213 ; Test RMSE = 0.9494\n",
      "Iteration: 60 ; Train RMSE = 0.9185 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9164 ; Test RMSE = 0.9471\n",
      "Iteration: 80 ; Train RMSE = 0.9147 ; Test RMSE = 0.9465\n",
      "Iteration: 90 ; Train RMSE = 0.9131 ; Test RMSE = 0.9460\n",
      "Iteration: 100 ; Train RMSE = 0.9115 ; Test RMSE = 0.9455\n"
     ]
    }
   ],
   "source": [
    "# 최적의 K 찾기\n",
    "results = []\n",
    "index = []\n",
    "\n",
    "R_temp = ratings.pivot(index='user_id',\n",
    "                        columns='movie_id',\n",
    "                        values='rating').fillna(0)\n",
    "for K in range(50, 261, 10):\n",
    "    print(f'K : {K}')\n",
    "    hyper_params = {\n",
    "        'K' : K,\n",
    "        'alpha' : 0.001,\n",
    "        'beta' : 0.02,\n",
    "        'iterations' : 100,\n",
    "        'verbose' : True\n",
    "    }\n",
    "    mf = NEW_MF(R_temp,\n",
    "                hyper_params)\n",
    "    test_set = mf.set_test(ratings_test)\n",
    "    result = mf.test()\n",
    "    index.append(K)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "for i in range(len(results)):\n",
    "    RMSE = []\n",
    "    for result in results[i]:\n",
    "        RMSE.append(result[2])\n",
    "    min = np.min(RMSE)\n",
    "    j = RMSE.index(min)\n",
    "    summary.append([index[i], j+1, RMSE[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(index, [x[2] for x in summary])\n",
    "plt.ylim(0.943, 0.9455)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c65b98e956c6ae24f8ae0bc56d1e465ff92310dbdec0a4bd6b48ffdf1441c98"
  },
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
