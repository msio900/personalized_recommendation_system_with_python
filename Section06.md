# ğŸ‘Section 06_ ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•œ ì¶”ì²œ ì‹œìŠ¤í…œ[â†©](../../)

## contentsğŸ“‘<a id='contents'></a>

* 0_ ë“¤ì–´ê°€ê¸° ì „ì—[âœï¸](#0)
* 1_ Maxtrix Factorization(MF)ì„ ì‹ ê²½ë§ìœ¼ë¡œ ë³€í™˜í•˜ê¸°[âœï¸](#0)

## 0_ ë“¤ì–´ê°€ê¸° ì „ì—[ğŸ“‘](#contents)<a id='0'></a>

* ë”¥ëŸ¬ë‹(Deep Learning : DL)ì€ ë‹¤ìˆ˜ì˜ ì€ë‹‰ì¸µ(hidden layer)ì„ ê°€ì§„ ì¸ê³µì‹ ê²½ë§ì„ ì ìš©í•œ ê¸°ë²•

## 1_ Maxtrix Factorization(MF)ì„ ì‹ ê²½ë§ìœ¼ë¡œ ë³€í™˜í•˜ê¸°[ğŸ“‘](#contents)<a id='1'></a>

![](./image/6_1-1.png)

* `MF`ë¥¼ `Keras`ë¡œ
* input layer : ê° ì‚¬ìš©ìì™€ itemìœ¼ë¡œ ë¶€í„° ì…ë ¥ì„ ë°›ëŠ” ë¶€ë¶„
  * one-hot representation : ì›í•«ì¸ì½”ë”©ê³¼ ê°™ìŒ. 1ì¸ì§€ 0ì¸ì§€ë¥¼ binaryí•œ í˜•íƒœë¡œ ë°”ê¿”ì¤Œ. 
* ì‚¬ìš©ìì˜ One-Hot Represention ì…ë ¥

|        | feature 1 | feature 2 | feature 3 | ...  | feature M |
| ------ | --------- | --------- | --------- | ---- | --------- |
| User 1 | 1         | 0         | 0         | 0    | 0         |
| User 2 | 0         | 1         | 0         | 0    | 0         |
| User 3 | 0         | 0         | 1         | 0    | 0         |
| ...    | 0         | 0         | 0         | 1    | 0         |
| User M | 0         | 0         | 0         | 0    | 1         |

* embedding Layer : ì ì¬ìš”ì¸ Kë¥¼ ê·œì •

  ![](./image/6_1-2.png)

  * ì‚¬ìš©ì ë…¸ë“œì— ëª¨ë‘ ì—°ê²°ë¨. ì¦‰, í™”ì‚´í‘œê°€ ê°¯ìˆ˜ê°€ M*Kê°œê°€ ì—°ê²°ë˜ì–´ ìˆìŒ. 
  * í•œ ì‚¬ìš©ìë‹¹ Kê°œì˜ í™”ì‚´í‘œê°€ ìˆìŒ.
  * ë§Œì•½ itemì´ë©´ N* Kê°€ ì—°ê²°ë˜ì–´ ìˆìŒ.

* MP : P(M * K)ì™€ Q(N * K)ê°€ ì—°ê²°ë¨. 

* Element-wise Product Layer

  ![](./image/6_1-3.png)

  * ì‚¬ìš©ìì™€ ì•„ì´í…œì˜ ê° í”„ë¡œë•íŠ¸ ì—°ì‚°ì„ ìœ„í•œ layer
  * P * Q<sup>T</sup>

* ì‚¬ìš©ìì™€ ì•„ì´í…œì˜ í‰ê°€ê²½í–¥(bias)

  ![](./image/6_1-4.png)

* ì´ ì •ë¦¬

  ![](./image/6_1-5.png)

  1. ì‚¬ìš©ì ì•„ì´í…œ ë‘ê°€ì§€ì˜ ì›í•« ë ˆí”„ë¦¬ì  í…Œì´ì…˜ì„ ì§„í–‰í•¨.
  2. ì‚¬ìš©ì ì…ë ¥ì€ Kê°œì˜ ë…¸ë“œë¥¼ ê°–ëŠ” ìœ ì €, ì•„ì´í…œ ì„ë² ë”©ê³¼ ì—°ê²°
  3. ìœ ì € ì„ë² ë”©ê³¼ ì•„ì´í…œ ì„ë² ë”©ì€ DOTí”„ë¡œë•íŠ¸ì—°ì‚°ìœ¼ë¡œ ì—°ê²°
  4. ë‹¤ì‹œ ì…ë ¥ìœ¼ë¡œ ëŒì•„ì™€ 1ê°œì˜ ì‚¬ìš©ìê°€ ê°ê° ì•„ì´í…œ í‰ê°€ ê²½í–¥, ìœ ì € í‰ê°€ ê²½í–¥ ì„ë² ë”©ê³¼ ì—°ê²°ë¨.
  5. ë§ˆì§€ë§‰ìœ¼ë¡œ ìœ ì €í‰ê°€ ê²½í–¥ ì„ë² ë”©, ì•„ì´í…œ í‰ê°€ ê²½í–¥ì„ë² ë”©, DOT ì´ ê²°í•¨ë¨. 
  6. Flatttenì€ ì°¨ì›ì„ ì¤„ì—¬ì£¼ëŠ” ì—­í• ì„ í•¨. 

* ìœ„ì—ì„œ BUì™€ BDëŠ” êµ¬í˜„ì„ í–ˆëŠ”ë°. B(ìƒìˆ˜)ë¥¼ êµ¬í˜„í•˜ì§„ ëª»í•¨.

  * ì „ì²´ í‰ê· ì„ ì¼ë¥ ì ìœ¼ë¡œ ë¹¼ì„œ í¸ì„±í•˜ê²Œ ë¨. 

## 2_ Kerasë¡œ MF êµ¬í˜„í•˜ê¸°[ğŸ“‘](#contents)<a id='2'></a>

* íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

  ```python
  # csv íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
  import  pandas as pd
  
  # train setê³¼ test setì„ ë‚˜ëˆ„ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
  from sklearn.model_selection import train_test_split
  
  # í•„ìš”í•œ tensorflow ëª¨ë“ˆë“¤ì„ ê°€ì ¸ì˜¨ë‹¤.
  import tensorflow as tf
  from tensorflow.keras import layers
  from tensorflow.keras.models import Model
  from tensorflow.keras.layers import Input, Embedding, Dot, Add, Flatten
  from tensorflow.keras.regularizers import l2
  from tensorflow.keras.optimizers import SGD, Adamax
  
  # DataFrame í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ì½ì–´ì˜¨ë‹¤.
  r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
  ratings = pd.read_csv('./Data/u.data',
                          names=r_cols,
                          sep='\t',
                          encoding='latin-1')
  
  ratings_train, ratings_test = train_test_split(ratings,
                                                  test_size=0.2,
                                                  shuffle=True,
                                                  random_state=2021)
  ```

* ë°ì´í„° ì„¤ì •

  ```python
  K = 200
  
  mu = ratings_train.rating.mean()
  
  M = ratings.user_id.max() + 1
  N = ratings.movie_id.max() + 1      # bias_comì˜ í¬ê¸° 1ì„ ê°ì•ˆí•˜ëŠ” ê²ƒ!
  
  def RMSE(y_true, y_pred):
      return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))
  ```

* P, Q ì‚¬ìš©ì í‰ê°€ ê²½í–¥ ì„ë² ë”©

  ```python
  user = Input(shape=(1,))
  item = Input(shape=(1,))
  
  P_embedding = Embedding(M, K, embeddings_regularizer=l2())(user)
  Q_embedding = Embedding(N, K, embeddings_regularizer=l2())(item)
  
  user_bias = Embedding(M, 1, embeddings_regularizer=l2())(user)
  item_bias = Embedding(N, 1, embeddings_regularizer=l2())(item)
  ```

* ìš”ì•½ ì¶œë ¥

  ```python
  R = layers.dot([P_embedding, Q_embedding], axes=(2,2)) # kì™€ kë¼ë¦¬ ì—°ì‚°ì„ í•˜ê² ë‹¤.
  
  R = layers.add([R, user_bias, item_bias])
  
  R = Flatten()(R)
  
  model = Model(inputs=[user, item], outputs=R)
  model.compile(
      loss=RMSE,
      optimizer=SGD(),
      metrics=[RMSE]
  )
  
  model.summary()
  
  # ì‹¤í–‰ ê²°ê³¼
  Model: "model"
  __________________________________________________________________________________________________
  Layer (type)                    Output Shape         Param #     Connected to                     
  ==================================================================================================
  input_3 (InputLayer)            [(None, 1)]          0                                            
  __________________________________________________________________________________________________
  input_4 (InputLayer)            [(None, 1)]          0                                            
  __________________________________________________________________________________________________
  embedding (Embedding)           (None, 1, 200)       188800      input_3[0][0]                    
  __________________________________________________________________________________________________
  embedding_1 (Embedding)         (None, 1, 200)       336600      input_4[0][0]                    
  __________________________________________________________________________________________________
  dot (Dot)                       (None, 1, 1)         0           embedding[0][0]                  
                                                                   embedding_1[0][0]                
  __________________________________________________________________________________________________
  embedding_2 (Embedding)         (None, 1, 1)         944         input_3[0][0]                    
  __________________________________________________________________________________________________
  embedding_3 (Embedding)         (None, 1, 1)         1683        input_4[0][0]                    
  __________________________________________________________________________________________________
  add (Add)                       (None, 1, 1)         0           dot[0][0]                        
                                                                   embedding_2[0][0]                
                                                                   embedding_3[0][0]                
  __________________________________________________________________________________________________
  flatten (Flatten)               (None, 1)            0           add[0][0]                        
  ==================================================================================================
  Total params: 528,027
  Trainable params: 528,027
  Non-trainable params: 0
  __________________________________________________________________________________________________
  
  ```

* ëª¨ë¸ ìƒì„±

  ```python
  result = model.fit(
      x = [ratings_train.user_id.values,
      ratings_train.movie_id.values],
      y = ratings_train.values - mu,
  
      epochs=60,
      batch_size=256,
      validation_data=(
          [ratings_test.user_id.values,
          ratings_test.movie_id.values],
          ratings_test.rating.values - mu
      )
  
  )
  
  # ì‹¤í–‰ ê²°ê³¼
  Epoch 1/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776128.0000 - val_loss: 5.0007 - val_RMSE: 1.1188
  Epoch 2/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776928.0000 - val_loss: 4.5440 - val_RMSE: 1.1186
  Epoch 3/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441776960.0000 - val_loss: 4.1412 - val_RMSE: 1.1185
  Epoch 4/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441776384.0000 - val_loss: 3.7860 - val_RMSE: 1.1184
  Epoch 5/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441775712.0000 - val_loss: 3.4727 - val_RMSE: 1.1184
  Epoch 6/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441776704.0000 - val_loss: 3.1965 - val_RMSE: 1.1183
  Epoch 7/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776544.0000 - val_loss: 2.9528 - val_RMSE: 1.1183
  Epoch 8/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441776704.0000 - val_loss: 2.7380 - val_RMSE: 1.1183
  Epoch 9/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441776544.0000 - val_loss: 2.5486 - val_RMSE: 1.1184
  Epoch 10/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441776512.0000 - val_loss: 2.3816 - val_RMSE: 1.1184
  Epoch 11/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776384.0000 - RMSE: 441777280.0000 - val_loss: 2.2343 - val_RMSE: 1.1184
  Epoch 12/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441775808.0000 - val_loss: 2.1044 - val_RMSE: 1.1185
  Epoch 13/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441776128.0000 - val_loss: 1.9900 - val_RMSE: 1.1185
  Epoch 14/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776128.0000 - RMSE: 441776928.0000 - val_loss: 1.8890 - val_RMSE: 1.1186
  Epoch 15/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441776192.0000 - val_loss: 1.8001 - val_RMSE: 1.1186
  Epoch 16/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441776512.0000 - val_loss: 1.7217 - val_RMSE: 1.1187
  Epoch 17/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776384.0000 - RMSE: 441776384.0000 - val_loss: 1.6525 - val_RMSE: 1.1188
  Epoch 18/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441775360.0000 - val_loss: 1.5916 - val_RMSE: 1.1188
  Epoch 19/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441775392.0000 - val_loss: 1.5379 - val_RMSE: 1.1189
  Epoch 20/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441776192.0000 - val_loss: 1.4906 - val_RMSE: 1.1190
  Epoch 21/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776384.0000 - RMSE: 441776288.0000 - val_loss: 1.4489 - val_RMSE: 1.1190
  Epoch 22/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776608.0000 - val_loss: 1.4122 - val_RMSE: 1.1191
  Epoch 23/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776512.0000 - RMSE: 441775968.0000 - val_loss: 1.3798 - val_RMSE: 1.1191
  Epoch 24/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776544.0000 - RMSE: 441775936.0000 - val_loss: 1.3513 - val_RMSE: 1.1192
  Epoch 25/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441775936.0000 - val_loss: 1.3262 - val_RMSE: 1.1193
  Epoch 26/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441776704.0000 - val_loss: 1.3041 - val_RMSE: 1.1193
  Epoch 27/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441777280.0000 - val_loss: 1.2846 - val_RMSE: 1.1194
  Epoch 28/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776608.0000 - RMSE: 441777248.0000 - val_loss: 1.2674 - val_RMSE: 1.1194
  Epoch 29/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441776224.0000 - val_loss: 1.2523 - val_RMSE: 1.1195
  Epoch 30/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776224.0000 - val_loss: 1.2391 - val_RMSE: 1.1195
  Epoch 31/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441777376.0000 - val_loss: 1.2274 - val_RMSE: 1.1196
  Epoch 32/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776512.0000 - RMSE: 441776448.0000 - val_loss: 1.2171 - val_RMSE: 1.1196
  Epoch 33/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441775712.0000 - val_loss: 1.2080 - val_RMSE: 1.1196
  Epoch 34/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441775968.0000 - val_loss: 1.2000 - val_RMSE: 1.1197
  Epoch 35/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776128.0000 - RMSE: 441776384.0000 - val_loss: 1.1930 - val_RMSE: 1.1197
  Epoch 36/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441776448.0000 - val_loss: 1.1869 - val_RMSE: 1.1197
  Epoch 37/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441776192.0000 - val_loss: 1.1814 - val_RMSE: 1.1198
  Epoch 38/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441777248.0000 - val_loss: 1.1767 - val_RMSE: 1.1198
  Epoch 39/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776128.0000 - RMSE: 441775648.0000 - val_loss: 1.1725 - val_RMSE: 1.1198
  Epoch 40/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776512.0000 - RMSE: 441776768.0000 - val_loss: 1.1688 - val_RMSE: 1.1199
  Epoch 41/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776128.0000 - RMSE: 441775360.0000 - val_loss: 1.1656 - val_RMSE: 1.1199
  Epoch 42/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776384.0000 - RMSE: 441776512.0000 - val_loss: 1.1628 - val_RMSE: 1.1199
  Epoch 43/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441776128.0000 - val_loss: 1.1603 - val_RMSE: 1.1200
  Epoch 44/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776128.0000 - val_loss: 1.1581 - val_RMSE: 1.1200
  Epoch 45/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776544.0000 - RMSE: 441776512.0000 - val_loss: 1.1562 - val_RMSE: 1.1200
  Epoch 46/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441775808.0000 - val_loss: 1.1545 - val_RMSE: 1.1200
  Epoch 47/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441775968.0000 - val_loss: 1.1530 - val_RMSE: 1.1200
  Epoch 48/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441776544.0000 - val_loss: 1.1517 - val_RMSE: 1.1201
  Epoch 49/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776512.0000 - RMSE: 441776384.0000 - val_loss: 1.1506 - val_RMSE: 1.1201
  Epoch 50/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441776512.0000 - val_loss: 1.1496 - val_RMSE: 1.1201
  Epoch 51/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441776224.0000 - val_loss: 1.1488 - val_RMSE: 1.1201
  Epoch 52/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441776032.0000 - val_loss: 1.1480 - val_RMSE: 1.1201
  Epoch 53/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776512.0000 - val_loss: 1.1474 - val_RMSE: 1.1201
  Epoch 54/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441775936.0000 - val_loss: 1.1468 - val_RMSE: 1.1201
  Epoch 55/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776512.0000 - RMSE: 441776768.0000 - val_loss: 1.1463 - val_RMSE: 1.1202
  Epoch 56/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441775936.0000 - val_loss: 1.1459 - val_RMSE: 1.1202
  Epoch 57/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776384.0000 - RMSE: 441776512.0000 - val_loss: 1.1455 - val_RMSE: 1.1202
  Epoch 58/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776768.0000 - val_loss: 1.1452 - val_RMSE: 1.1202
  Epoch 59/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441776352.0000 - val_loss: 1.1449 - val_RMSE: 1.1202
  Epoch 60/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441776224.0000 - val_loss: 1.1446 - val_RMSE: 1.1202
  ```

* RMSE ì‹œê°í™”

  ```python
  # plot RMSE
  import matplotlib.pyplot as plt
  
  plt.plot(result.history['RMSE'], label="Train RMSE")
  plt.plot(result.history['val_RMSE'], label = 'Test RMSE')
  plt.legend()
  plt.show()
  ```

  ![](./image/6_2.png)
