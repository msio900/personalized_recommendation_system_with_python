# ğŸ‘Section 06_ ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•œ ì¶”ì²œ ì‹œìŠ¤í…œ[â†©](../../)

## contentsğŸ“‘<a id='contents'></a>

* 0_ ë“¤ì–´ê°€ê¸° ì „ì—[âœï¸](#0)
* 1_ Maxtrix Factorization(MF)ì„ ì‹ ê²½ë§ìœ¼ë¡œ ë³€í™˜í•˜ê¸°[âœï¸](#1)
* 2_ Kerasë¡œ MF êµ¬í˜„í•˜ê¸°[âœï¸](#2)
* 3_ ë”¥ëŸ¬ë‹ì„ ì ìš©í•œ ì¶”ì²œ ì‹œìŠ¤í…œ[âœï¸](#3)

## 0_ ë“¤ì–´ê°€ê¸° ì „ì—[ğŸ“‘](#contents)<a id='0'></a>

* ë”¥ëŸ¬ë‹(Deep Learning : DL)ì€ ë‹¤ìˆ˜ì˜ ì€ë‹‰ì¸µ(hidden layer)ì„ ê°€ì§„ ì¸ê³µì‹ ê²½ë§ì„ ì ìš©í•œ ê¸°ë²•

## 1_ Maxtrix Factorization(MF)ì„ ì‹ ê²½ë§ìœ¼ë¡œ ë³€í™˜í•˜ê¸°[ğŸ“‘](#contents)<a id='1'></a>

![](./image/6_1-1.png)

* `MF`ë¥¼ `Keras`ë¡œ
* input layer : ê° ì‚¬ìš©ìì™€ itemìœ¼ë¡œ ë¶€í„° ì…ë ¥ì„ ë°›ëŠ” ë¶€ë¶„
  * one-hot representation : ì›í•«ì¸ì½”ë”©ê³¼ ê°™ìŒ. 1ì¸ì§€ 0ì¸ì§€ë¥¼ binaryí•œ í˜•íƒœë¡œ ë°”ê¿”ì¤Œ. 
* ì‚¬ìš©ìì˜ One-Hot Represention ì…ë ¥

|        | feature 1 | feature 2 | feature 3 | ...  | feature M |
| ------ | --------- | --------- | --------- | ---- | --------- |
| User 1 | 1         | 0         | 0         | 0    | 0         |
| User 2 | 0         | 1         | 0         | 0    | 0         |
| User 3 | 0         | 0         | 1         | 0    | 0         |
| ...    | 0         | 0         | 0         | 1    | 0         |
| User M | 0         | 0         | 0         | 0    | 1         |

* embedding Layer : ì ì¬ìš”ì¸ Kë¥¼ ê·œì •

  ![](./image/6_1-2.png)

  * ì‚¬ìš©ì ë…¸ë“œì— ëª¨ë‘ ì—°ê²°ë¨. ì¦‰, í™”ì‚´í‘œê°€ ê°¯ìˆ˜ê°€ M*Kê°œê°€ ì—°ê²°ë˜ì–´ ìˆìŒ. 
  * í•œ ì‚¬ìš©ìë‹¹ Kê°œì˜ í™”ì‚´í‘œê°€ ìˆìŒ.
  * ë§Œì•½ itemì´ë©´ N* Kê°€ ì—°ê²°ë˜ì–´ ìˆìŒ.

* MP : P(M * K)ì™€ Q(N * K)ê°€ ì—°ê²°ë¨. 

* Element-wise Product Layer

  ![](./image/6_1-3.png)

  * ì‚¬ìš©ìì™€ ì•„ì´í…œì˜ ê° í”„ë¡œë•íŠ¸ ì—°ì‚°ì„ ìœ„í•œ layer
  * P * Q<sup>T</sup>

* ì‚¬ìš©ìì™€ ì•„ì´í…œì˜ í‰ê°€ê²½í–¥(bias)

  ![](./image/6_1-4.png)

* ì´ ì •ë¦¬

  ![](./image/6_1-5.png)

  1. ì‚¬ìš©ì ì•„ì´í…œ ë‘ê°€ì§€ì˜ ì›í•« ë ˆí”„ë¦¬ì  í…Œì´ì…˜ì„ ì§„í–‰í•¨.
  2. ì‚¬ìš©ì ì…ë ¥ì€ Kê°œì˜ ë…¸ë“œë¥¼ ê°–ëŠ” ìœ ì €, ì•„ì´í…œ ì„ë² ë”©ê³¼ ì—°ê²°
  3. ìœ ì € ì„ë² ë”©ê³¼ ì•„ì´í…œ ì„ë² ë”©ì€ DOTí”„ë¡œë•íŠ¸ì—°ì‚°ìœ¼ë¡œ ì—°ê²°
  4. ë‹¤ì‹œ ì…ë ¥ìœ¼ë¡œ ëŒì•„ì™€ 1ê°œì˜ ì‚¬ìš©ìê°€ ê°ê° ì•„ì´í…œ í‰ê°€ ê²½í–¥, ìœ ì € í‰ê°€ ê²½í–¥ ì„ë² ë”©ê³¼ ì—°ê²°ë¨.
  5. ë§ˆì§€ë§‰ìœ¼ë¡œ ìœ ì €í‰ê°€ ê²½í–¥ ì„ë² ë”©, ì•„ì´í…œ í‰ê°€ ê²½í–¥ì„ë² ë”©, DOT ì´ ê²°í•¨ë¨. 
  6. Flatttenì€ ì°¨ì›ì„ ì¤„ì—¬ì£¼ëŠ” ì—­í• ì„ í•¨. 

* ìœ„ì—ì„œ BUì™€ BDëŠ” êµ¬í˜„ì„ í–ˆëŠ”ë°. B(ìƒìˆ˜)ë¥¼ êµ¬í˜„í•˜ì§„ ëª»í•¨.

  * ì „ì²´ í‰ê· ì„ ì¼ë¥ ì ìœ¼ë¡œ ë¹¼ì„œ í¸ì„±í•˜ê²Œ ë¨. 

## 2_ Kerasë¡œ MF êµ¬í˜„í•˜ê¸°[ğŸ“‘](#contents)<a id='2'></a>

* íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

  ```python
  # csv íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
  import  pandas as pd
  
  # train setê³¼ test setì„ ë‚˜ëˆ„ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
  from sklearn.model_selection import train_test_split
  
  # í•„ìš”í•œ tensorflow ëª¨ë“ˆë“¤ì„ ê°€ì ¸ì˜¨ë‹¤.
  import tensorflow as tf
  from tensorflow.keras import layers
  from tensorflow.keras.models import Model
  from tensorflow.keras.layers import Input, Embedding, Dot, Add, Flatten
  from tensorflow.keras.regularizers import l2
  from tensorflow.keras.optimizers import SGD, Adamax
  
  # DataFrame í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ì½ì–´ì˜¨ë‹¤.
  r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
  ratings = pd.read_csv('./Data/u.data',
                          names=r_cols,
                          sep='\t',
                          encoding='latin-1')
  
  ratings_train, ratings_test = train_test_split(ratings,
                                                  test_size=0.2,
                                                  shuffle=True,
                                                  random_state=2021)
  ```

* ë°ì´í„° ì„¤ì •

  ```python
  K = 200
  
  mu = ratings_train.rating.mean()
  
  M = ratings.user_id.max() + 1
  N = ratings.movie_id.max() + 1      # bias_comì˜ í¬ê¸° 1ì„ ê°ì•ˆí•˜ëŠ” ê²ƒ!
  
  def RMSE(y_true, y_pred):
      return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))
  ```

* P, Q ì‚¬ìš©ì í‰ê°€ ê²½í–¥ ì„ë² ë”©

  ```python
  user = Input(shape=(1,))
  item = Input(shape=(1,))
  
  P_embedding = Embedding(M, K, embeddings_regularizer=l2())(user)
  Q_embedding = Embedding(N, K, embeddings_regularizer=l2())(item)
  
  user_bias = Embedding(M, 1, embeddings_regularizer=l2())(user)
  item_bias = Embedding(N, 1, embeddings_regularizer=l2())(item)
  ```

* ìš”ì•½ ì¶œë ¥

  ```python
  R = layers.dot([P_embedding, Q_embedding], axes=(2,2)) # kì™€ kë¼ë¦¬ ì—°ì‚°ì„ í•˜ê² ë‹¤.
  
  R = layers.add([R, user_bias, item_bias])
  
  R = Flatten()(R)
  
  model = Model(inputs=[user, item], outputs=R)
  model.compile(
      loss=RMSE,
      optimizer=SGD(),
      metrics=[RMSE]
  )
  
  model.summary()
  
  # ì‹¤í–‰ ê²°ê³¼
  Model: "model"
  __________________________________________________________________________________________________
  Layer (type)                    Output Shape         Param #     Connected to                     
  ==================================================================================================
  input_3 (InputLayer)            [(None, 1)]          0                                            
  __________________________________________________________________________________________________
  input_4 (InputLayer)            [(None, 1)]          0                                            
  __________________________________________________________________________________________________
  embedding (Embedding)           (None, 1, 200)       188800      input_3[0][0]                    
  __________________________________________________________________________________________________
  embedding_1 (Embedding)         (None, 1, 200)       336600      input_4[0][0]                    
  __________________________________________________________________________________________________
  dot (Dot)                       (None, 1, 1)         0           embedding[0][0]                  
                                                                   embedding_1[0][0]                
  __________________________________________________________________________________________________
  embedding_2 (Embedding)         (None, 1, 1)         944         input_3[0][0]                    
  __________________________________________________________________________________________________
  embedding_3 (Embedding)         (None, 1, 1)         1683        input_4[0][0]                    
  __________________________________________________________________________________________________
  add (Add)                       (None, 1, 1)         0           dot[0][0]                        
                                                                   embedding_2[0][0]                
                                                                   embedding_3[0][0]                
  __________________________________________________________________________________________________
  flatten (Flatten)               (None, 1)            0           add[0][0]                        
  ==================================================================================================
  Total params: 528,027
  Trainable params: 528,027
  Non-trainable params: 0
  __________________________________________________________________________________________________
  
  ```

* ëª¨ë¸ ìƒì„±

  ```python
  result = model.fit(
      x = [ratings_train.user_id.values,
      ratings_train.movie_id.values],
      y = ratings_train.values - mu,
  
      epochs=60,
      batch_size=256,
      validation_data=(
          [ratings_test.user_id.values,
          ratings_test.movie_id.values],
          ratings_test.rating.values - mu
      )
  
  )
  
  # ì‹¤í–‰ ê²°ê³¼
  Epoch 1/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776128.0000 - val_loss: 5.0007 - val_RMSE: 1.1188
  Epoch 2/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776928.0000 - val_loss: 4.5440 - val_RMSE: 1.1186
  Epoch 3/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441776960.0000 - val_loss: 4.1412 - val_RMSE: 1.1185
  Epoch 4/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441776384.0000 - val_loss: 3.7860 - val_RMSE: 1.1184
  Epoch 5/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441775712.0000 - val_loss: 3.4727 - val_RMSE: 1.1184
  Epoch 6/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441776704.0000 - val_loss: 3.1965 - val_RMSE: 1.1183
  Epoch 7/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776544.0000 - val_loss: 2.9528 - val_RMSE: 1.1183
  Epoch 8/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441776704.0000 - val_loss: 2.7380 - val_RMSE: 1.1183
  Epoch 9/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441776544.0000 - val_loss: 2.5486 - val_RMSE: 1.1184
  Epoch 10/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441776512.0000 - val_loss: 2.3816 - val_RMSE: 1.1184
  Epoch 11/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776384.0000 - RMSE: 441777280.0000 - val_loss: 2.2343 - val_RMSE: 1.1184
  Epoch 12/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441775808.0000 - val_loss: 2.1044 - val_RMSE: 1.1185
  Epoch 13/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441776128.0000 - val_loss: 1.9900 - val_RMSE: 1.1185
  Epoch 14/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776128.0000 - RMSE: 441776928.0000 - val_loss: 1.8890 - val_RMSE: 1.1186
  Epoch 15/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441776192.0000 - val_loss: 1.8001 - val_RMSE: 1.1186
  Epoch 16/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441776512.0000 - val_loss: 1.7217 - val_RMSE: 1.1187
  Epoch 17/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776384.0000 - RMSE: 441776384.0000 - val_loss: 1.6525 - val_RMSE: 1.1188
  Epoch 18/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441775360.0000 - val_loss: 1.5916 - val_RMSE: 1.1188
  Epoch 19/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441775392.0000 - val_loss: 1.5379 - val_RMSE: 1.1189
  Epoch 20/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441776192.0000 - val_loss: 1.4906 - val_RMSE: 1.1190
  Epoch 21/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776384.0000 - RMSE: 441776288.0000 - val_loss: 1.4489 - val_RMSE: 1.1190
  Epoch 22/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776608.0000 - val_loss: 1.4122 - val_RMSE: 1.1191
  Epoch 23/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776512.0000 - RMSE: 441775968.0000 - val_loss: 1.3798 - val_RMSE: 1.1191
  Epoch 24/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776544.0000 - RMSE: 441775936.0000 - val_loss: 1.3513 - val_RMSE: 1.1192
  Epoch 25/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441775936.0000 - val_loss: 1.3262 - val_RMSE: 1.1193
  Epoch 26/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441776704.0000 - val_loss: 1.3041 - val_RMSE: 1.1193
  Epoch 27/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441777280.0000 - val_loss: 1.2846 - val_RMSE: 1.1194
  Epoch 28/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776608.0000 - RMSE: 441777248.0000 - val_loss: 1.2674 - val_RMSE: 1.1194
  Epoch 29/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441776224.0000 - val_loss: 1.2523 - val_RMSE: 1.1195
  Epoch 30/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776224.0000 - val_loss: 1.2391 - val_RMSE: 1.1195
  Epoch 31/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441777376.0000 - val_loss: 1.2274 - val_RMSE: 1.1196
  Epoch 32/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776512.0000 - RMSE: 441776448.0000 - val_loss: 1.2171 - val_RMSE: 1.1196
  Epoch 33/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441775712.0000 - val_loss: 1.2080 - val_RMSE: 1.1196
  Epoch 34/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441775968.0000 - val_loss: 1.2000 - val_RMSE: 1.1197
  Epoch 35/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776128.0000 - RMSE: 441776384.0000 - val_loss: 1.1930 - val_RMSE: 1.1197
  Epoch 36/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441776448.0000 - val_loss: 1.1869 - val_RMSE: 1.1197
  Epoch 37/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441776192.0000 - val_loss: 1.1814 - val_RMSE: 1.1198
  Epoch 38/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441777248.0000 - val_loss: 1.1767 - val_RMSE: 1.1198
  Epoch 39/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776128.0000 - RMSE: 441775648.0000 - val_loss: 1.1725 - val_RMSE: 1.1198
  Epoch 40/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776512.0000 - RMSE: 441776768.0000 - val_loss: 1.1688 - val_RMSE: 1.1199
  Epoch 41/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776128.0000 - RMSE: 441775360.0000 - val_loss: 1.1656 - val_RMSE: 1.1199
  Epoch 42/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776384.0000 - RMSE: 441776512.0000 - val_loss: 1.1628 - val_RMSE: 1.1199
  Epoch 43/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441776128.0000 - val_loss: 1.1603 - val_RMSE: 1.1200
  Epoch 44/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776128.0000 - val_loss: 1.1581 - val_RMSE: 1.1200
  Epoch 45/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776544.0000 - RMSE: 441776512.0000 - val_loss: 1.1562 - val_RMSE: 1.1200
  Epoch 46/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441775808.0000 - val_loss: 1.1545 - val_RMSE: 1.1200
  Epoch 47/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776224.0000 - RMSE: 441775968.0000 - val_loss: 1.1530 - val_RMSE: 1.1200
  Epoch 48/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441776544.0000 - val_loss: 1.1517 - val_RMSE: 1.1201
  Epoch 49/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776512.0000 - RMSE: 441776384.0000 - val_loss: 1.1506 - val_RMSE: 1.1201
  Epoch 50/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441776512.0000 - val_loss: 1.1496 - val_RMSE: 1.1201
  Epoch 51/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776352.0000 - RMSE: 441776224.0000 - val_loss: 1.1488 - val_RMSE: 1.1201
  Epoch 52/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441776032.0000 - val_loss: 1.1480 - val_RMSE: 1.1201
  Epoch 53/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776512.0000 - val_loss: 1.1474 - val_RMSE: 1.1201
  Epoch 54/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776448.0000 - RMSE: 441775936.0000 - val_loss: 1.1468 - val_RMSE: 1.1201
  Epoch 55/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776512.0000 - RMSE: 441776768.0000 - val_loss: 1.1463 - val_RMSE: 1.1202
  Epoch 56/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441775936.0000 - val_loss: 1.1459 - val_RMSE: 1.1202
  Epoch 57/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776384.0000 - RMSE: 441776512.0000 - val_loss: 1.1455 - val_RMSE: 1.1202
  Epoch 58/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776288.0000 - RMSE: 441776768.0000 - val_loss: 1.1452 - val_RMSE: 1.1202
  Epoch 59/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441776352.0000 - val_loss: 1.1449 - val_RMSE: 1.1202
  Epoch 60/60
  313/313 [==============================] - 1s 3ms/step - loss: 441776192.0000 - RMSE: 441776224.0000 - val_loss: 1.1446 - val_RMSE: 1.1202
  ```

* RMSE ì‹œê°í™”

  ```python
  # plot RMSE
  import matplotlib.pyplot as plt
  
  plt.plot(result.history['RMSE'], label="Train RMSE")
  plt.plot(result.history['val_RMSE'], label = 'Test RMSE')
  plt.legend()
  plt.show()
  ```

  ![](./image/6_2.png)

* ì ìš©í•´ë³´ê¸°

  ```python
  user_ids = ratings_test.user_id.values[0:6] # ì ìš©í•´ë³´ê¸°
  movie_ids = ratings_test.movie_id.values[0:6]
  
  predictions = model.predict([user_ids, movie_ids]) + mu #ì „ì²´ í‰ê·  ë‹¤ì‹œ ë”í•˜ê¸°
  # ì‹¤ì œ ê°’
  print(ratings_test[0:6])
         user_id  movie_id  rating  timestamp
  23307      468        51       3  875293386
  36679       92       780       3  875660494
  36626      555       489       5  879975455
  83753      940        69       2  885921265
  52604      181      1350       1  878962120
  49877      320       195       5  884749255
  # ì˜ˆì¸¡ ê°’
  print(predictions)
  [[3.5550046]
   [3.4723089]
   [3.5492196]
   [3.5690255]
   [3.1871848]
   [3.5966241]]
  ```

* RMSE ì„¤ì •

  ```python
  import numpy as np
  
  def RMSE2(y_true, y_pred):
    return np.sqrt(np.mean((np.array(y_true)-np.array(y_pred))**2))
  ```

* RMSE ê³„ì‚°

  ```python
  user_ids = ratings_test.user_id.values
  movie_ids = ratings_test.movie_id.values
  
  y_pred = model.predict([user_ids, movie_ids]) + mu
  y_pred = np.ravel(y_pred,order="C") #1ì°¨ì› í˜•íƒœë¡œ ë°”ê¿”ì¤Œ.
  
  y_true = np.array(ratings_test.rating)
  
  RMSE2(y_true, y_pred)
  
  # ì‹¤í–‰ ê²°ê³¼
  1.0913778530076552
  ```

  * ìµœì í™”ë‚˜ íŠœë‹ì´ ì•ˆë˜ì–´ ìˆê¸°ë•Œë¬¸ì— ì„±ëŠ¥ì´ ì¢‹ì§€ ì•ŠìŒ.

## 3_ ë”¥ëŸ¬ë‹ì„ ì ìš©í•œ ì¶”ì²œ ì‹œìŠ¤í…œ[ğŸ“‘](#contents)<a id='3'></a>

![](./image/6_3-1.png)

* RAW data ë¥¼ embedding
* ì´ì „ê³¼ ë‹¤ë¥¸ ì ì€ `ì€ë‹‰ì¸µ`(hidden layer)ë¥¼ ì¶”ê°€í•¨.

![](./image/6_3-2.png)

* layer1 : user latent vector + item latent vector = `concanating`(ê²°í•©)ì„ ì´ìš©

* êµ¬í˜„

  ```python
  import pandas as pd
  import numpy as np
  
  #train setê³¼ test setì„ ë‚˜ëˆ„ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
  from sklearn.model_selection import train_test_split
  
  #í•„ìš”í•œ tensorflow ëª¨ë“ˆë“¤ì„ ê°€ì ¸ì˜¨ë‹¤.
  import tensorflow as tf
  from tensorflow.keras import layers
  from tensorflow.keras.models import Model
  from tensorflow.keras.layers import Input, Embedding, Dot, Add, Flatten
  from tensorflow.keras.regularizers import l2
  from tensorflow.keras.optimizers import SGD, Adamax
  
  #layer êµ¬ì„±ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° 
  from tensorflow.keras.layers import Dense, Concatenate, Activation # ì´ ë¶€ë¶„ì€ ë ˆì´ì–´ êµ¬ì„±ì„ ìœ„í•´ í•„ìš”í•œ ë¦¬ì•„ë¸ŒëŸ¬ë¦¬
  from tensorflow.keras.regularizers import l2 
  from tensorflow.keras.optimizers import SGD, Adamax
  
  #DataFrame í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ì½ì–´ì˜¨ë‹¤.
  r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
  ratings = pd.read_csv('./Data/u.data', names=r_cols,  sep='\t',encoding='latin-1')
  
  ratings_train, ratings_test = train_test_split(ratings,
                                                 test_size = 0.2,
                                                 shuffle = True,
                                                 random_state = 2021)
  ### Defining RMSE measure ###
  # y_true, y_predì€ ì‹ ê²½ë§ì—ì„œ ì‹¤ì œê°’, ì˜ˆì¸¡ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” Tensorflow/Keras í‘œì¤€ ë³€ìˆ˜
  def RMSE(y_true, y_pred):
    # Tensorflowì˜ mathí´ë˜ìŠ¤ì— ë¯¸ë¦¬ ì •ì˜ëœ
    # ì œê³±ê·¼(sqrt), í‰ê· (reduce_mean), ì œê³±(square) í•¨ìˆ˜ë¥¼ í†µí•´ RMSE ê³„ì‚°
    return tf.sqrt(tf.reduce_mean(tf.square(y_true-y_pred)))
  
  ### Variable ì´ˆê¸°í™” ###
  #ì ì¬ìš”ì¸ ìˆ˜ 200ìœ¼ë¡œ ì§€ì •í•œë‹¤.
  K = 200
  
  #ì „ì²´ í‰ê·  ê³„ì‚°í•œë‹¤.
  mu = ratings_train.rating.mean()
  
  #ì‚¬ìš©ì ì•„ì´ë””ì™€ ì˜í™” ì•„ì´ë””ì˜ ìµœëŒ“ê°’ -> ë³´í†µì€ uniqueí•œ ê°’ì˜ ê°œìˆ˜ + 1ë¡œ í•´ì•¼í•¨ 
  #1ì„ ë”í•˜ëŠ” ì´ìœ  : bias term ì¶”ê°€ ê³ ë ¤
  M = ratings.user_id.max() + 1
  N = ratings.movie_id.max() + 1
  
  #################################################################################
  #kreas ëª¨ë¸ 
  # ì•„ë˜ë¶€ë¶„ì€ ì•ì—ì„œì™€ ë™ì¼í•˜ê²Œ ì‚¬ìš©ìì™€ ì•„ì´í…œ ë°ì´í„°ë¥¼ embeddingì„ í†µí•´
  # ê°ê° Kê°œì˜ ë…¸ë“œë¥¼ ê°–ëŠ” layerë¡œ ë³€í™˜í•˜ê³ 
  # ì‚¬ìš©ì biasì™€ ì•„ì´í…œ biasë¥¼ 1ê°œì˜ ë…¸ë“œë¥¼ ê°–ëŠ” layerë¡œ ë³€í™˜í•œë‹¤.
  user = Input(shape=(1,))
  item = Input(shape=(1,))
  
  #Embedding
  P_embedding = Embedding(M,K,embeddings_regularizer=l2())(user) #regularizer : ê·œì œ -> ê³¼ì í•© ë°©ì§€
  Q_embedding = Embedding(N,K,embeddings_regularizer=l2())(item)
  
  #bias
  user_bias = Embedding(M,1,embeddings_regularizer=l2())(user)
  item_bias = Embedding(N,1,embeddings_regularizer=l2())(item)
  
  #ì•ê³¼ ë’¤ë¥¼ í•œì¤„ë¡œ ë¶™ì´ê¸°
  # ì´ë¥¼ ìœ„í•´ì„œ 1ìí˜•ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë§Œë“¦.
  P_embedding = Flatten()(P_embedding)
  Q_embedding = Flatten()(Q_embedding)
  user_bias = Flatten()(user_bias)
  item_bias = Flatten()(item_bias)
  
  R = Concatenate()([P_embedding, Q_embedding, user_bias, item_bias])
  
  R = Dense(2048)(R) #ë…¸ë“œê°€ 2048ê°œì¸ í•˜ë‚˜ì˜ layerë¥¼ ë§Œë“  í›„ Rê³¼ ì—°ê²° 
  R = Activation('linear')(R) # ìœ„ì— ìˆëŠ” Denseë ˆì´ì–´ë¥¼ activation functionì„ ì§€ì •í•¨.
  
  R = Dense(256)(R)
  R = Activation('linear')(R)
  
  R = Dense(1)(R) #ì¶œë ¥ layer
  
  model = Model(inputs = [user,item],outputs=R)
  
  model.compile(loss=RMSE,
      optimizer=SGD(), #Adamaxë„ ê°€ëŠ¥
      metrics = [RMSE]
  )
  
  model.summary()
  
  # ì‹¤í–‰ ê²°ê³¼
  Model: "model_1"
  __________________________________________________________________________________________________
  Layer (type)                    Output Shape         Param #     Connected to                     
  ==================================================================================================
  input_3 (InputLayer)            [(None, 1)]          0                                            
  __________________________________________________________________________________________________
  input_4 (InputLayer)            [(None, 1)]          0                                            
  __________________________________________________________________________________________________
  embedding_4 (Embedding)         (None, 1, 200)       188800      input_3[0][0]                    
  __________________________________________________________________________________________________
  embedding_5 (Embedding)         (None, 1, 200)       336600      input_4[0][0]                    
  __________________________________________________________________________________________________
  embedding_6 (Embedding)         (None, 1, 1)         944         input_3[0][0]                    
  __________________________________________________________________________________________________
  embedding_7 (Embedding)         (None, 1, 1)         1683        input_4[0][0]                    
  __________________________________________________________________________________________________
  flatten_1 (Flatten)             (None, 200)          0           embedding_4[0][0]                
  __________________________________________________________________________________________________
  flatten_2 (Flatten)             (None, 200)          0           embedding_5[0][0]                
  __________________________________________________________________________________________________
  flatten_3 (Flatten)             (None, 1)            0           embedding_6[0][0]                
  __________________________________________________________________________________________________
  flatten_4 (Flatten)             (None, 1)            0           embedding_7[0][0]                
  __________________________________________________________________________________________________
  concatenate (Concatenate)       (None, 402)          0           flatten_1[0][0]                  
                                                                   flatten_2[0][0]                  
                                                                   flatten_3[0][0]                  
                                                                   flatten_4[0][0]                  
  __________________________________________________________________________________________________
  dense (Dense)                   (None, 2048)         825344      concatenate[0][0]                
  __________________________________________________________________________________________________
  activation (Activation)         (None, 2048)         0           dense[0][0]                      
  __________________________________________________________________________________________________
  dense_1 (Dense)                 (None, 256)          524544      activation[0][0]                 
  __________________________________________________________________________________________________
  activation_1 (Activation)       (None, 256)          0           dense_1[0][0]                    
  __________________________________________________________________________________________________
  dense_2 (Dense)                 (None, 1)            257         activation_1[0][0]               
  ==================================================================================================
  Total params: 1,878,172
  Trainable params: 1,878,172
  Non-trainable params: 0
  __________________________________________________________________________________________________
  ```

* ëª¨ë¸ êµ¬ì„±

  ```python
  # Model fitting
  # ëª¨ë¸ ì…ë ¥ì— í•„ìš”í•œ ë°ì´í„° ì •ë¦¬
  train_user_ids = ratings_train.user_id.values
  train_movie_ids = ratings_train.movie_id.values
  train_ratings = ratings_train.rating.values
  
  test_user_ids = ratings_test.user_id.values
  test_movie_ids = ratings_test.movie_id.values
  test_ratings = ratings_test.rating.values
  
  #ì‹ ê²½ë§ í•™ìŠµ
  result = model.fit(
      x = [train_user_ids, train_movie_ids],
      y = train_ratings - mu, #ì „ì²´ í‰ê·  ë¹¼ê¸° 
      epochs = 65,
      batch_size = 512, #batch_size : ì „ì²´ train_setì—ì„œ 512ê°œì”© í•™ìŠµì‹œí‚¤ê² ë‹¤.
      validation_data = (
          [test_user_ids, test_movie_ids],
           test_ratings - mu
      )
  )
  
  # ì‹¤í–‰ ê²°ê³¼
  Epoch 1/65
  157/157 [==============================] - 4s 20ms/step - loss: 5.3929 - RMSE: 1.1267 - val_loss: 5.2497 - val_RMSE: 1.1177
  Epoch 2/65
  157/157 [==============================] - 3s 20ms/step - loss: 5.1316 - RMSE: 1.1248 - val_loss: 4.9967 - val_RMSE: 1.1162
  Epoch 3/65
  157/157 [==============================] - 3s 21ms/step - loss: 4.8863 - RMSE: 1.1233 - val_loss: 4.7592 - val_RMSE: 1.1147
  Epoch 4/65
  157/157 [==============================] - 3s 20ms/step - loss: 4.6559 - RMSE: 1.1225 - val_loss: 4.5361 - val_RMSE: 1.1133
  Epoch 5/65
  157/157 [==============================] - 3s 20ms/step - loss: 4.4394 - RMSE: 1.1207 - val_loss: 4.3265 - val_RMSE: 1.1119
  Epoch 6/65
  157/157 [==============================] - 3s 20ms/step - loss: 4.2360 - RMSE: 1.1188 - val_loss: 4.1296 - val_RMSE: 1.1104
  Epoch 7/65
  157/157 [==============================] - 3s 21ms/step - loss: 4.0449 - RMSE: 1.1173 - val_loss: 3.9444 - val_RMSE: 1.1087
  Epoch 8/65
  157/157 [==============================] - 3s 20ms/step - loss: 3.8653 - RMSE: 1.1156 - val_loss: 3.7702 - val_RMSE: 1.1069
  Epoch 9/65
  157/157 [==============================] - 3s 20ms/step - loss: 3.6962 - RMSE: 1.1134 - val_loss: 3.6065 - val_RMSE: 1.1049
  Epoch 10/65
  157/157 [==============================] - 3s 21ms/step - loss: 3.5371 - RMSE: 1.1115 - val_loss: 3.4523 - val_RMSE: 1.1025
  Epoch 11/65
  157/157 [==============================] - 3s 21ms/step - loss: 3.3874 - RMSE: 1.1087 - val_loss: 3.3073 - val_RMSE: 1.1000
  Epoch 12/65
  157/157 [==============================] - 3s 21ms/step - loss: 3.2463 - RMSE: 1.1057 - val_loss: 3.1704 - val_RMSE: 1.0967
  Epoch 13/65
  157/157 [==============================] - 3s 20ms/step - loss: 3.1132 - RMSE: 1.1021 - val_loss: 3.0417 - val_RMSE: 1.0935
  Epoch 14/65
  157/157 [==============================] - 3s 20ms/step - loss: 2.9875 - RMSE: 1.0984 - val_loss: 2.9194 - val_RMSE: 1.0890
  Epoch 15/65
  157/157 [==============================] - 3s 20ms/step - loss: 2.8687 - RMSE: 1.0941 - val_loss: 2.8041 - val_RMSE: 1.0840
  Epoch 16/65
  157/157 [==============================] - 3s 20ms/step - loss: 2.7562 - RMSE: 1.0881 - val_loss: 2.6950 - val_RMSE: 1.0785
  Epoch 17/65
  157/157 [==============================] - 3s 20ms/step - loss: 2.6496 - RMSE: 1.0825 - val_loss: 2.5915 - val_RMSE: 1.0721
  Epoch 18/65
  157/157 [==============================] - 3s 20ms/step - loss: 2.5483 - RMSE: 1.0756 - val_loss: 2.4933 - val_RMSE: 1.0648
  Epoch 19/65
  157/157 [==============================] - 3s 20ms/step - loss: 2.4521 - RMSE: 1.0669 - val_loss: 2.4002 - val_RMSE: 1.0571
  Epoch 20/65
  157/157 [==============================] - 3s 20ms/step - loss: 2.3606 - RMSE: 1.0582 - val_loss: 2.3107 - val_RMSE: 1.0476
  Epoch 21/65
  157/157 [==============================] - 3s 20ms/step - loss: 2.2735 - RMSE: 1.0490 - val_loss: 2.2264 - val_RMSE: 1.0381
  Epoch 22/65
  157/157 [==============================] - 3s 20ms/step - loss: 2.1910 - RMSE: 1.0396 - val_loss: 2.1465 - val_RMSE: 1.0285
  Epoch 23/65
  157/157 [==============================] - 3s 20ms/step - loss: 2.1125 - RMSE: 1.0295 - val_loss: 2.0711 - val_RMSE: 1.0191
  Epoch 24/65
  157/157 [==============================] - 3s 21ms/step - loss: 2.0385 - RMSE: 1.0192 - val_loss: 1.9992 - val_RMSE: 1.0092
  Epoch 25/65
  157/157 [==============================] - 3s 21ms/step - loss: 1.9688 - RMSE: 1.0090 - val_loss: 1.9322 - val_RMSE: 1.0005
  Epoch 26/65
  157/157 [==============================] - 3s 21ms/step - loss: 1.9033 - RMSE: 1.0003 - val_loss: 1.8695 - val_RMSE: 0.9925
  Epoch 27/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.8419 - RMSE: 0.9922 - val_loss: 1.8103 - val_RMSE: 0.9849
  Epoch 28/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.7844 - RMSE: 0.9845 - val_loss: 1.7553 - val_RMSE: 0.9784
  Epoch 29/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.7305 - RMSE: 0.9781 - val_loss: 1.7042 - val_RMSE: 0.9729
  Epoch 30/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.6803 - RMSE: 0.9723 - val_loss: 1.6562 - val_RMSE: 0.9679
  Epoch 31/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.6334 - RMSE: 0.9662 - val_loss: 1.6110 - val_RMSE: 0.9632
  Epoch 32/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.5894 - RMSE: 0.9619 - val_loss: 1.5692 - val_RMSE: 0.9592
  Epoch 33/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.5483 - RMSE: 0.9578 - val_loss: 1.5323 - val_RMSE: 0.9582
  Epoch 34/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.5099 - RMSE: 0.9537 - val_loss: 1.4935 - val_RMSE: 0.9528
  Epoch 35/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.4740 - RMSE: 0.9503 - val_loss: 1.4595 - val_RMSE: 0.9505
  Epoch 36/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.4402 - RMSE: 0.9471 - val_loss: 1.4266 - val_RMSE: 0.9473
  Epoch 37/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.4087 - RMSE: 0.9441 - val_loss: 1.3974 - val_RMSE: 0.9460
  Epoch 38/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.3793 - RMSE: 0.9421 - val_loss: 1.3688 - val_RMSE: 0.9435
  Epoch 39/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.3515 - RMSE: 0.9398 - val_loss: 1.3424 - val_RMSE: 0.9418
  Epoch 40/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.3257 - RMSE: 0.9378 - val_loss: 1.3174 - val_RMSE: 0.9399
  Epoch 41/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.3014 - RMSE: 0.9355 - val_loss: 1.2954 - val_RMSE: 0.9397
  Epoch 42/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.2786 - RMSE: 0.9342 - val_loss: 1.2726 - val_RMSE: 0.9373
  Epoch 43/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.2575 - RMSE: 0.9331 - val_loss: 1.2521 - val_RMSE: 0.9361
  Epoch 44/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.2375 - RMSE: 0.9320 - val_loss: 1.2359 - val_RMSE: 0.9378
  Epoch 45/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.2187 - RMSE: 0.9306 - val_loss: 1.2158 - val_RMSE: 0.9349
  Epoch 46/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.2011 - RMSE: 0.9296 - val_loss: 1.1984 - val_RMSE: 0.9334
  Epoch 47/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.1849 - RMSE: 0.9282 - val_loss: 1.1827 - val_RMSE: 0.9328
  Epoch 48/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.1696 - RMSE: 0.9287 - val_loss: 1.1705 - val_RMSE: 0.9348
  Epoch 49/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.1549 - RMSE: 0.9272 - val_loss: 1.1541 - val_RMSE: 0.9316
  Epoch 50/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.1416 - RMSE: 0.9269 - val_loss: 1.1438 - val_RMSE: 0.9338
  Epoch 51/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.1289 - RMSE: 0.9261 - val_loss: 1.1300 - val_RMSE: 0.9317
  Epoch 52/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.1171 - RMSE: 0.9258 - val_loss: 1.1184 - val_RMSE: 0.9313
  Epoch 53/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.1059 - RMSE: 0.9253 - val_loss: 1.1068 - val_RMSE: 0.9301
  Epoch 54/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.0952 - RMSE: 0.9245 - val_loss: 1.0984 - val_RMSE: 0.9313
  Epoch 55/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.0856 - RMSE: 0.9243 - val_loss: 1.0873 - val_RMSE: 0.9296
  Epoch 56/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.0763 - RMSE: 0.9249 - val_loss: 1.0819 - val_RMSE: 0.9328
  Epoch 57/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.0676 - RMSE: 0.9237 - val_loss: 1.0702 - val_RMSE: 0.9292
  Epoch 58/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.0595 - RMSE: 0.9235 - val_loss: 1.0627 - val_RMSE: 0.9292
  Epoch 59/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.0521 - RMSE: 0.9234 - val_loss: 1.0547 - val_RMSE: 0.9286
  Epoch 60/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.0446 - RMSE: 0.9233 - val_loss: 1.0482 - val_RMSE: 0.9289
  Epoch 61/65
  157/157 [==============================] - 3s 21ms/step - loss: 1.0379 - RMSE: 0.9234 - val_loss: 1.0424 - val_RMSE: 0.9293
  Epoch 62/65
  157/157 [==============================] - 3s 21ms/step - loss: 1.0319 - RMSE: 0.9235 - val_loss: 1.0352 - val_RMSE: 0.9282
  Epoch 63/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.0259 - RMSE: 0.9232 - val_loss: 1.0344 - val_RMSE: 0.9333
  Epoch 64/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.0203 - RMSE: 0.9233 - val_loss: 1.0244 - val_RMSE: 0.9284
  Epoch 65/65
  157/157 [==============================] - 3s 20ms/step - loss: 1.0147 - RMSE: 0.9227 - val_loss: 1.0206 - val_RMSE: 0.9297
  ```

* ìµœì ì˜ ëª¨ë¸ ê·¸ë˜í”„

  ```python
  #plot RMSE
  import matplotlib.pyplot as plt 
  plt.plot(result.history['RMSE'], label = 'Train RMSE')
  plt.plot(result.history['val_RMSE'], label = 'Test RMSE')
  plt.legend()
  plt.show()
  ```

  ![](./image/6_3-3.png)
