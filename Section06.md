# ğŸ‘Section 06_ ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•œ ì¶”ì²œ ì‹œìŠ¤í…œ[â†©](../../)

## contentsğŸ“‘<a id='contents'></a>

* 0_ ë“¤ì–´ê°€ê¸° ì „ì—[âœï¸](#0)
* 1_ Maxtrix Factorization(MF)ì„ ì‹ ê²½ë§ìœ¼ë¡œ ë³€í™˜í•˜ê¸°[âœï¸](#0)

## 0_ ë“¤ì–´ê°€ê¸° ì „ì—[ğŸ“‘](#contents)<a id='0'></a>

* ë”¥ëŸ¬ë‹(Deep Learning : DL)ì€ ë‹¤ìˆ˜ì˜ ì€ë‹‰ì¸µ(hidden layer)ì„ ê°€ì§„ ì¸ê³µì‹ ê²½ë§ì„ ì ìš©í•œ ê¸°ë²•

## 1_ Maxtrix Factorization(MF)ì„ ì‹ ê²½ë§ìœ¼ë¡œ ë³€í™˜í•˜ê¸°[ğŸ“‘](#contents)<a id='1'></a>

![](./image/6_1-1.png)

* `MF`ë¥¼ `Keras`ë¡œ
* input layer : ê° ì‚¬ìš©ìì™€ itemìœ¼ë¡œ ë¶€í„° ì…ë ¥ì„ ë°›ëŠ” ë¶€ë¶„
  * one-hot representation : ì›í•«ì¸ì½”ë”©ê³¼ ê°™ìŒ. 1ì¸ì§€ 0ì¸ì§€ë¥¼ binaryí•œ í˜•íƒœë¡œ ë°”ê¿”ì¤Œ. 
* ì‚¬ìš©ìì˜ One-Hot Represention ì…ë ¥

|        | feature 1 | feature 2 | feature 3 | ...  | feature M |
| ------ | --------- | --------- | --------- | ---- | --------- |
| User 1 | 1         | 0         | 0         | 0    | 0         |
| User 2 | 0         | 1         | 0         | 0    | 0         |
| User 3 | 0         | 0         | 1         | 0    | 0         |
| ...    | 0         | 0         | 0         | 1    | 0         |
| User M | 0         | 0         | 0         | 0    | 1         |

* embedding Layer : ì ì¬ìš”ì¸ Kë¥¼ ê·œì •

  ![](./image/6_1-2.png)

  * ì‚¬ìš©ì ë…¸ë“œì— ëª¨ë‘ ì—°ê²°ë¨. ì¦‰, í™”ì‚´í‘œê°€ ê°¯ìˆ˜ê°€ M*Kê°œê°€ ì—°ê²°ë˜ì–´ ìˆìŒ. 
  * í•œ ì‚¬ìš©ìë‹¹ Kê°œì˜ í™”ì‚´í‘œê°€ ìˆìŒ.
  * ë§Œì•½ itemì´ë©´ N* Kê°€ ì—°ê²°ë˜ì–´ ìˆìŒ.

* MP : P(M * K)ì™€ Q(N * K)ê°€ ì—°ê²°ë¨. 

* Element-wise Product Layer

  ![](./image/6_1-3.png)

  * ì‚¬ìš©ìì™€ ì•„ì´í…œì˜ ê° í”„ë¡œë•íŠ¸ ì—°ì‚°ì„ ìœ„í•œ layer
  * P * Q<sup>T</sup>

* ì‚¬ìš©ìì™€ ì•„ì´í…œì˜ í‰ê°€ê²½í–¥(bias)

  ![](./image/6_1-4.png)

* ì´ ì •ë¦¬

  ![](./image/6_1-5.png)

  1. ì‚¬ìš©ì ì•„ì´í…œ ë‘ê°€ì§€ì˜ ì›í•« ë ˆí”„ë¦¬ì  í…Œì´ì…˜ì„ ì§„í–‰í•¨.
  2. ì‚¬ìš©ì ì…ë ¥ì€ Kê°œì˜ ë…¸ë“œë¥¼ ê°–ëŠ” ìœ ì €, ì•„ì´í…œ ì„ë² ë”©ê³¼ ì—°ê²°
  3. ìœ ì € ì„ë² ë”©ê³¼ ì•„ì´í…œ ì„ë² ë”©ì€ DOTí”„ë¡œë•íŠ¸ì—°ì‚°ìœ¼ë¡œ ì—°ê²°
  4. ë‹¤ì‹œ ì…ë ¥ìœ¼ë¡œ ëŒì•„ì™€ 1ê°œì˜ ì‚¬ìš©ìê°€ ê°ê° ì•„ì´í…œ í‰ê°€ ê²½í–¥, ìœ ì € í‰ê°€ ê²½í–¥ ì„ë² ë”©ê³¼ ì—°ê²°ë¨.
  5. ë§ˆì§€ë§‰ìœ¼ë¡œ ìœ ì €í‰ê°€ ê²½í–¥ ì„ë² ë”©, ì•„ì´í…œ í‰ê°€ ê²½í–¥ì„ë² ë”©, DOT ì´ ê²°í•¨ë¨. 
  6. Flatttenì€ ì°¨ì›ì„ ì¤„ì—¬ì£¼ëŠ” ì—­í• ì„ í•¨. 

* ìœ„ì—ì„œ BUì™€ BDëŠ” êµ¬í˜„ì„ í–ˆëŠ”ë°. B(ìƒìˆ˜)ë¥¼ êµ¬í˜„í•˜ì§„ ëª»í•¨.

  * ì „ì²´ í‰ê· ì„ ì¼ë¥ ì ìœ¼ë¡œ ë¹¼ì„œ í¸ì„±í•˜ê²Œ ë¨. 

## 2_ Kerasë¡œ MF êµ¬í˜„í•˜ê¸°[ğŸ“‘](#contents)<a id='2'></a>

* íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

  ```python
  # csv íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
  import  pandas as pd
  
  # train setê³¼ test setì„ ë‚˜ëˆ„ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
  from sklearn.model_selection import train_test_split
  
  # í•„ìš”í•œ tensorflow ëª¨ë“ˆë“¤ì„ ê°€ì ¸ì˜¨ë‹¤.
  import tensorflow as tf
  from tensorflow.keras import layers
  from tensorflow.keras.models import Model
  from tensorflow.keras.layers import Input, Embedding, Dot, Add, Flatten
  from tensorflow.keras.regularizers import l2
  from tensorflow.keras.optimizers import SGD, Adamax
  
  # DataFrame í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ì½ì–´ì˜¨ë‹¤.
  r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
  ratings = pd.read_csv('./Data/u.data',
                          names=r_cols,
                          sep='\t',
                          encoding='latin-1')
  
  ratings_train, ratings_test = train_test_split(ratings,
                                                  test_size=0.2,
                                                  shuffle=True,
                                                  random_state=2021)
  ```

* ë°ì´í„° ì„¤ì •

  ```python
  K = 200
  
  mu = ratings_train.rating.mean()
  
  M = ratings.user_id.max() + 1
  N = ratings.movie_id.max() + 1      # bias_comì˜ í¬ê¸° 1ì„ ê°ì•ˆí•˜ëŠ” ê²ƒ!
  
  def RMSE(y_true, y_pred):
      return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))
  ```

* P, Q ì‚¬ìš©ì í‰ê°€ ê²½í–¥ ì„ë² ë”©

  ```python
  user = Input(shape=(1,))
  item = Input(shape=(1,))
  
  P_embedding = Embedding(M, K, embeddings_regularizer=l2())(user)
  Q_embedding = Embedding(N, K, embeddings_regularizer=l2())(item)
  
  user_bias = Embedding(M, 1, embeddings_regularizer=l2())(user)
  item_bias = Embedding(N, 1, embeddings_regularizer=l2())(item)
  ```

* ìš”ì•½ ì¶œë ¥

  ```python
  R = layers.dot([P_embedding, Q_embedding], axes=(2,2))
  
  R = layers.add([R, user_bias, item_bias])
  
  R = Flatten()(R)
  
  model = Model(inputs=[user, item], outputs=R)
  model.compile(
      loss=RMSE,
      optimizer=SGD(),
      metrics=[RMSE]
  )
  
  model.summary()
  
  # ì‹¤í–‰ ê²°ê³¼
  Model: "model"
  __________________________________________________________________________________________________
  Layer (type)                    Output Shape         Param #     Connected to                     
  ==================================================================================================
  input_3 (InputLayer)            [(None, 1)]          0                                            
  __________________________________________________________________________________________________
  input_4 (InputLayer)            [(None, 1)]          0                                            
  __________________________________________________________________________________________________
  embedding (Embedding)           (None, 1, 200)       188800      input_3[0][0]                    
  __________________________________________________________________________________________________
  embedding_1 (Embedding)         (None, 1, 200)       336600      input_4[0][0]                    
  __________________________________________________________________________________________________
  dot (Dot)                       (None, 1, 1)         0           embedding[0][0]                  
                                                                   embedding_1[0][0]                
  __________________________________________________________________________________________________
  embedding_2 (Embedding)         (None, 1, 1)         944         input_3[0][0]                    
  __________________________________________________________________________________________________
  embedding_3 (Embedding)         (None, 1, 1)         1683        input_4[0][0]                    
  __________________________________________________________________________________________________
  add (Add)                       (None, 1, 1)         0           dot[0][0]                        
                                                                   embedding_2[0][0]                
                                                                   embedding_3[0][0]                
  __________________________________________________________________________________________________
  flatten (Flatten)               (None, 1)            0           add[0][0]                        
  ==================================================================================================
  Total params: 528,027
  Trainable params: 528,027
  Non-trainable params: 0
  __________________________________________________________________________________________________
  
  ```

  
